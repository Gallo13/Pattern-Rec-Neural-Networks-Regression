{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created by: Jessica Gallo\n",
    "\n",
    "# Created: 3/8/2020\n",
    "# Last Modified: 3/25/2020\n",
    "\n",
    "# CSC 732 Pattern Recognition and Neural Networks\n",
    "# Regression (Linear, Multiple, Quadratic, Cubic etc.)\n",
    "# Using Logistic Regression for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------\n",
    "# IMPORTS |\n",
    "# --------\n",
    "# Main Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visual Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn Libraries\n",
    "from sklearn.impute import SimpleImputer\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# from sklearn.svm import SVR\n",
    "# import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "=========\n",
      "      age     sex     bmi  children smoker     region      charges\n",
      "0      19  female  27.900         0    yes  southwest  16884.92400\n",
      "1      18    male  33.770         1     no  southeast   1725.55230\n",
      "2      28    male  33.000         3     no  southeast   4449.46200\n",
      "3      33    male  22.705         0     no  northwest  21984.47061\n",
      "4      32    male  28.880         0     no  northwest   3866.85520\n",
      "...   ...     ...     ...       ...    ...        ...          ...\n",
      "1333   50    male  30.970         3     no  northwest  10600.54830\n",
      "1334   18  female  31.920         0     no  northeast   2205.98080\n",
      "1335   18  female  36.850         0     no  southeast   1629.83350\n",
      "1336   21  female  25.800         0     no  southwest   2007.94500\n",
      "1337   61  female  29.070         0    yes  northwest  29141.36030\n",
      "\n",
      "[1338 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# ------------------\n",
    "# INSURANCE DATASET |\n",
    "# ------------------\n",
    "dataset = pd.read_csv(\"C:\\\\Users\\\\User\\\\Downloads\\\\insurance.csv\")\n",
    "# reading dataset\n",
    "X = dataset.iloc[:, :-1].values\n",
    "# X represents a matrix of independent variables\n",
    "# the 1st ':' stands for all rows\n",
    "# the second ':' stands for all the columns minus the last one (-1)\n",
    "y = dataset.iloc[:, 6].values\n",
    "# y represents a vector of the dependent variable\n",
    "# all rows included, but from the columns we only need the 7th (6th index)\n",
    "\n",
    "print('Original:'\n",
    "      '\\n=========')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# DATA PREPROCESSING |\n",
    "# ==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imputed:\n",
      "========\n",
      "[[19 'female' 27.9 0 'yes' 'southwest']\n",
      " [18 'male' 33.77 1 'no' 'southeast']\n",
      " [28 'male' 33.0 3 'no' 'southeast']\n",
      " ...\n",
      " [18 'female' 36.85 0 'no' 'southeast']\n",
      " [21 'female' 25.8 0 'no' 'southwest']\n",
      " [61 'female' 29.07 0 'yes' 'northwest']]\n"
     ]
    }
   ],
   "source": [
    "# -------------\n",
    "# MISSING DATA |\n",
    "# -------------\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='constant')\n",
    "# handles missing data & replaces NaN values\n",
    "# strategy argument 'constant' replaces missing values with fill_value (for string/object datatypes)\n",
    "imputer = imputer.fit(X[:, 1:6])\n",
    "# fits the imputer on X\n",
    "# # fits data to avoid data leakage during cross validation\n",
    "X[:, 1:6] = imputer.transform(X[:, 1:6])\n",
    "# imputes all missing values in X\n",
    "\n",
    "print(\"\\nImputed:\"\n",
    "      \"\\n========\")\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoded:\n",
      "========\n",
      "[[0. 1. 0. ... 0. 0. 1.]\n",
      " [1. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# CONVERT CATEGORICAL TEXT DATA INTO MODEL-UNDERSTANDABLE NUMBERICAL DATA |\n",
    "# ------------------------------------------------------------------------\n",
    "labelencoder_X = LabelEncoder()\n",
    "# encodes target lables with values between 0 and n_classes -1\n",
    "X[:, 0] = labelencoder_X.fit_transform(X[:, 0])\n",
    "# fit label encoder and return encoded labels\n",
    "onehotencoder = OneHotEncoder(dtype=np.float)\n",
    "# creates a binary column for each category and returns a sparse matrix or dense array\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "# fit OneHotEncoder to X, then transform X\n",
    "\n",
    "print('\\nEncoded:'\n",
    "      '\\n========')\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting Dataset:\n",
      "==================\n",
      "X_train: \n",
      " [[0. 0. 0. ... 1. 0. 0.]\n",
      " [1. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      "y_train: \n",
      " [ 8410.04685  11884.04858  40904.1995    3861.20965  37079.372\n",
      "  4058.1161    2395.17155   1880.07     12629.1656    4718.20355\n",
      " 11741.726     8556.907    26236.57997   2866.091     6406.4107\n",
      "  3393.35635  11830.6072    9174.13565   2254.7967   62592.87309\n",
      "  3947.4131    8027.968    46130.5265    9778.3472   10107.2206\n",
      " 20745.9891    1708.0014    3171.6149    4934.705    42211.1382\n",
      "  4438.2634    6571.02435   1702.4553    7345.084     7196.867\n",
      " 10564.8845   17352.6803    8965.79575   6203.90175   4889.9995\n",
      " 10579.711    11833.7823    2775.19215   8211.1002   43943.8761\n",
      "  9850.432     8457.818     5240.765     6238.298     2020.5523\n",
      "  2566.4707   16085.1275   17560.37975   5031.26955  13880.949\n",
      " 23967.38305  12928.7911    6117.4945    7726.854     7153.5539\n",
      "  5989.52365   3481.868     4518.82625  36197.699     6282.235\n",
      "  1621.8827    3906.127     1242.26     20630.28351  14283.4594\n",
      "  3857.75925  10806.839    11987.1682   36149.4835   48885.13561\n",
      " 11289.10925  11842.442     1121.8739   12495.29085   4320.41085\n",
      "  8733.22925  12622.1795    5693.4305    4005.4225   10269.46\n",
      " 24059.68019  38711.        1720.3537   17904.52705   1252.407\n",
      "  4779.6023    8059.6791    3756.6216    1708.92575   1906.35825\n",
      " 21344.8467   12981.3457   28101.33305  13390.559    11534.87265\n",
      "  4751.07     26467.09737   8302.53565  12029.2867    7348.142\n",
      " 11082.5772   12032.326    28476.73499  39722.7462    5209.57885\n",
      "  9414.92     39727.614     7222.78625  23807.2406    3554.203\n",
      " 10602.385    42111.6647    6746.7425    6652.5288    1633.0444\n",
      " 30166.61817  26018.95052   5709.1644   13224.693    23568.272\n",
      "  2585.85065   9288.0267    6500.2359    3176.8159   13770.0979\n",
      " 20420.60465   3227.1211   40273.6455   23082.95533  14410.9321\n",
      "  3056.3881    3766.8838    6435.6237    1256.299    27808.7251\n",
      "  4243.59005   8269.044     4466.6214    2102.2647    1622.1885\n",
      " 10106.13425  14256.1928   33732.6867    2850.68375   5373.36425\n",
      " 23065.4207    6796.86325   4504.6624    9283.562     7325.0482\n",
      "  9386.1613   44641.1974    3238.4357    5630.45785   5272.1758\n",
      " 13228.84695   8827.2099    8605.3615   10231.4999   19350.3689\n",
      " 38511.6283    3693.428    10422.91665   4189.1131   33475.81715\n",
      " 10791.96      5584.3057   12146.971     5257.50795  13844.7972\n",
      "  7537.1639   11013.7119   13747.87235   4399.731     6653.7886\n",
      "  6748.5912   13725.47184   9361.3268   19515.5416    7731.4271\n",
      " 14692.66935   3292.52985  10797.3362   10976.24575  13129.60345\n",
      " 10355.641    14455.64405   6311.952    11299.343    32108.66282\n",
      "  7633.7206    7077.1894    8252.2843    4618.0799    8515.7587\n",
      "  6849.026     2045.68525  12224.35085   4500.33925  12797.20962\n",
      " 47305.305     4719.52405  36898.73308   1621.3402   25309.489\n",
      " 11837.16     48173.361    10977.2063    6948.7008    7201.70085\n",
      " 40720.55105  35491.64     28923.13692  11090.7178    1682.597\n",
      "  2731.9122    4337.7352    3847.674    33307.5508    1909.52745\n",
      " 21472.4788    7729.64575  33907.548     7640.3092   23887.6627\n",
      " 19442.3535   19521.9682   44423.803     3392.9768   17043.3414\n",
      "  8823.98575  48970.2476   14382.70905   8062.764    13393.756\n",
      "  3761.292     3392.3652    1131.5066   29523.1656   32548.3405\n",
      "  3925.7582    3279.86855  32734.1863    2150.469    34806.4677\n",
      " 13844.506    17878.90068  12957.118     5662.225    12574.049\n",
      "  7623.518    27346.04207  13887.204     8442.667     5926.846\n",
      "  8116.68      5397.6167   34472.841     5920.1041    3277.161\n",
      "  7443.64305   2899.48935  40003.33225   5261.46945  24476.47851\n",
      " 17663.1442    2680.9493    3471.4096    5729.0053   11945.1327\n",
      " 25333.33284  10338.9316    4837.5823   39983.42595   6571.544\n",
      " 11735.87905   8964.06055  20167.33603   9504.3103    6985.50695\n",
      "  4719.73655  13457.9608    5377.4578    3353.284     2480.9791\n",
      " 15006.57945  10197.7722    6198.7518    5125.2157   27218.43725\n",
      "  1634.5734   41919.097    16776.30405   4894.7533    2196.4732\n",
      " 11394.06555  30284.64294   1972.95      2201.0971    4883.866\n",
      "  2257.47525  11436.73815   2211.13075   5312.16985   1137.011\n",
      " 16657.71745  39836.519    14426.07385  41661.602    10704.47\n",
      "  2974.126    28468.91901  27117.99378   1705.6245    8825.086\n",
      "  6551.7501   11552.904    12982.8747    5855.9025   24393.6224\n",
      "  4239.89265  42560.4304    8703.456    14394.5579    2203.73595\n",
      "  2322.6218   48549.17835   9182.17     19933.458    24671.66334\n",
      "  4340.4409   14451.83515  26125.67477  20296.86345  36580.28216\n",
      "  9875.6804   34254.05335   1728.897     8347.1643    9377.9047\n",
      " 22218.1149    2198.18985  12333.828    11165.41765  26926.5144\n",
      "  5354.07465   7209.4918    3206.49135   2138.0707    1515.3449\n",
      "  9411.005     6112.35295   8604.48365  47896.79135  11345.519\n",
      " 20984.0936   43578.9394   46151.1245   45702.02235  44585.45587\n",
      "  3378.91      5910.944    14358.36437  37742.5757   39597.4072\n",
      "  5266.3656   12890.05765  26392.26029  18767.7377    3161.454\n",
      "  8334.45755   7742.1098    7445.918    34166.273     4544.2348\n",
      "  9748.9106   12129.61415   1837.2819    2842.76075   2927.0647\n",
      " 39611.7577   18223.4512   11729.6795   19040.876     3213.62205\n",
      "  7419.4779    4746.344    21880.82      2200.83085   8125.7845\n",
      " 10825.2537   13126.67745   2331.519     8603.8234    5028.1466\n",
      " 46599.1084    4949.7587    4234.927     3935.1799   13143.86485\n",
      " 13405.3903    5615.369     2498.4144    6837.3687   13974.45555\n",
      " 29186.48236  12925.886     8627.5411   16884.924     8891.1395\n",
      " 10600.5483   23241.47453   9788.8659    1815.8759    7626.993\n",
      "  1842.519     3410.324    21082.16      8116.26885   9095.06825\n",
      " 16297.846     1526.312     3591.48      6184.2994    7256.7231\n",
      "  5116.5004    4032.2407   22478.6       4830.63      2137.6536\n",
      " 12323.936     7151.092     2803.69785   9058.7303   10923.9332\n",
      "  3180.5101   51194.55914   1532.4697    3044.2133   13555.0049\n",
      " 11305.93455   7682.67      3021.80915   4185.0979   38245.59327\n",
      " 12913.9924    8334.5896   12646.207    14474.675     5383.536\n",
      " 11658.37915   2221.56445   6875.961    13451.122     1664.9996\n",
      " 14478.33015   2494.022     9866.30485  10156.7832    2302.3\n",
      "  6356.2707   34303.1672    7727.2532   15230.32405  32787.45859\n",
      "  1241.565     2730.10785   7261.741     4922.9159   25656.57526\n",
      "  8978.1851    5425.02335   2913.569    21659.9301    6250.435\n",
      "  6185.3208   18765.87545  11931.12525   6455.86265  39774.2763\n",
      " 19798.05455   4747.0529   42969.8527    2166.732    36219.40545\n",
      "  2464.6188   14901.5167    2217.46915   5002.7827    1694.7964\n",
      "  1627.28245  15161.5344    3972.9247   35585.576     8671.19125\n",
      " 23045.56616   9855.1314   12485.8009    8277.523    21771.3423\n",
      " 39241.442    10065.413     6496.886    13217.0945    9625.92\n",
      "  4687.797     5974.3847    8428.0693   24667.419    17929.30337\n",
      "  1877.9294    2104.1134    3201.24515  24227.33724   7147.4728\n",
      "  9282.4806    9872.701     7624.63     23401.30575  21595.38229\n",
      " 19719.6947   42856.838    12105.32     21978.6769   13112.6048\n",
      " 13415.0381   20773.62775  11381.3254    4877.98105  47496.49445\n",
      "  6770.1925    9724.53      1704.70015  38792.6856    4762.329\n",
      " 10461.9794    4266.1658   46889.2612    9048.0273   11015.1747\n",
      "  2457.21115  30942.1918    5478.0368   21232.18226   8534.6718\n",
      " 39047.285     1711.0268    2205.9808   46718.16325   9391.346\n",
      " 11033.6617   11396.9002    4527.18295  16796.41194  11848.141\n",
      "  5245.2269    5400.9805   44501.3982    7731.85785  19673.33573\n",
      "  1875.344    10601.412    10325.206     3268.84665   8944.1151\n",
      " 24915.22085  11272.33139  12363.547    19964.7463   15170.069\n",
      " 23306.547     1986.9334    1674.6323   10407.08585  16115.3045\n",
      "  3597.596     2473.3341   35160.13457   5594.8455    5458.04645\n",
      " 11093.6229   39871.7043   41949.2441    5124.1887   42760.5022\n",
      " 48673.5588    5267.81815   6799.458     2710.82855   4260.744\n",
      "  4846.92015   8023.13545  10085.846     9144.565     7265.7025\n",
      "  2217.6012    6858.4796   35147.52848   5148.5526    6710.1919\n",
      "  7789.635    37607.5277   13430.265     4667.60765  12829.4551\n",
      " 10226.2842    3989.841    16450.8947   19444.2658    1631.6683\n",
      " 43921.1837   34439.8559   47403.88     18804.7524   10072.05505\n",
      " 37484.4493   15820.699    15612.19335  17496.306    13831.1152\n",
      " 11840.77505   8413.46305  12592.5345    2689.4954   16420.49455\n",
      " 23288.9284   14449.8544   11454.0215    5484.4673   18838.70366\n",
      "  2741.948     2721.3208    6877.9801   12222.8983    7050.0213\n",
      " 14711.7438    3757.8448    7358.17565   9620.3307   22144.032\n",
      " 46255.1125   46661.4424   13204.28565   5757.41345   2055.3249\n",
      " 10115.00885  36189.1017    8823.279    63770.42801  17626.23951\n",
      " 11365.952     9634.538     4571.41305   1712.227     7986.47525\n",
      "  8596.8278   10959.33     11244.3769    4435.0942    4350.5144\n",
      " 10577.087    12523.6048    9583.8933    2801.2588   15359.1045\n",
      " 13470.8044   36085.219     4753.6368    9301.89355  48675.5177\n",
      "  6067.12675   1727.785    10594.2257    6123.5688   40419.0191\n",
      " 11657.7189   33900.653    21677.28345  10702.6424    8688.85885\n",
      "  6272.4772   10043.249    45008.9555   36397.576    17748.5062\n",
      " 48824.45     14988.432    11326.71487   4561.1885    4433.3877\n",
      " 27533.9129    3561.8889   13063.883    14007.222    13224.05705\n",
      "  1704.5681   55135.40209   5472.449    12815.44495   4562.8421\n",
      " 16069.08475  10807.4863    5836.5204    1163.4627   12648.7034\n",
      " 13429.0354   41999.52      2632.992    14418.2804    8232.6388\n",
      "  1759.338    11881.9696    8538.28845   3659.346     3875.7341\n",
      " 27375.90478   4463.2051   30184.9367   27322.73386   4347.02335\n",
      "  5138.2567    6358.77645  19107.7796   10982.5013   19144.57652\n",
      "  7337.748    36910.60803   1880.487    11743.299     7173.35995\n",
      " 21259.37795   2438.0552    3309.7926   12643.3778   11150.78\n",
      "  2304.0022   34838.873     7228.21565  25517.11363  13635.6379\n",
      "  2719.27975  18963.17192   2007.945    47055.5321   39556.4945\n",
      " 28868.6639    5649.715    17468.9839   18648.4217    5012.471\n",
      " 24513.09126   3062.50825  13981.85035   9566.9909   38344.566\n",
      "  2020.177    16586.49771   4296.2712    2117.33885   8835.26495\n",
      "  9877.6077    1769.53165  11944.59435   3484.331    36950.2567\n",
      " 36124.5737    6113.23105   4564.19145   1964.78      7133.9025\n",
      "  9447.25035   4766.022     2404.7338    7162.0122    4915.05985\n",
      "  1135.9407    4661.28635   4462.7218    3176.2877    4673.3922\n",
      "  6373.55735   6781.3542    1744.465     7441.501     1967.0227\n",
      "  3490.5491    4149.736     1391.5287    1748.774    15518.18025\n",
      "  9880.068     9549.5651    6600.361     5428.7277    8162.71625\n",
      " 37133.8982    7323.734819  1629.8335   18310.742     4151.0287\n",
      "  5325.651    11411.685     7160.094     1607.5101   12475.3513\n",
      "  7512.267     3577.999     2154.361     9869.8102   46113.511\n",
      "  2362.22905  26109.32905   8516.829     8601.3293    7985.815\n",
      " 17179.522    19199.944     6548.19505   7345.7266   16455.70785\n",
      "  9541.69555  10096.97     38126.2465   18955.22017   5708.867\n",
      "  9617.66245   7518.02535  25382.297     4906.40965  28950.4692\n",
      "  5979.731     3956.07145   8615.3      17085.2676    3046.062\n",
      "  8782.469     9193.8385   12979.358     2902.9065    2534.39375\n",
      " 11363.2832   37165.1638    5385.3379   11566.30055  12430.95335\n",
      " 44400.4064    2855.43755   1261.442     5253.524    37829.7242\n",
      "  6082.405   ]\n",
      "\n",
      "X_test: \n",
      " [[0. 1. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 1. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "y_test: \n",
      " [ 1646.4297  11353.2276   8798.593   10381.4787   2103.08    38746.3551\n",
      "  9304.7019  11658.11505  3070.8087  19539.243   12629.8967  11538.421\n",
      "  6338.0756   7050.642    1137.4697   8968.33    21984.47061  6414.178\n",
      " 28287.89766 13462.52     9722.7695  40932.4295   8026.6666   8444.474\n",
      "  2203.47185  6664.68595  8606.2174   8283.6807   5375.038    3645.0894\n",
      " 11674.13    11737.84884 24873.3849  33750.2918  24180.9335   9863.4718\n",
      " 36837.467   17942.106   11856.4115  39725.51805  4349.462   11743.9341\n",
      " 19749.38338 12347.172    4931.647   30259.99556 27724.28875 34672.1472\n",
      "  9644.2525  14394.39815 12557.6053  11881.358    2352.96845  9101.798\n",
      " 17178.6824   3994.1778  40941.2854  12644.589   22395.74424  1149.3959\n",
      "  3366.6697  13143.33665 18328.2381   2690.1138  12741.16745  8765.249\n",
      " 10264.4421  22192.43711  2709.24395 14571.8908  60021.39897 58571.07448\n",
      "  1743.214   12479.70895 13352.0998  41034.2214   2789.0574   2867.1196\n",
      " 11070.535   10493.9458   3167.45585 12269.68865 10942.13205  1977.815\n",
      " 39125.33225 42112.2356   7045.499   46200.9851  10370.91255  7749.1564\n",
      " 20234.85475 24106.91255 13919.8229   8871.1517   1628.4709   9487.6442\n",
      "  9861.025   12265.5069  12235.8392   6402.29135 11356.6609  15817.9857\n",
      " 18259.216   34779.615   27037.9141  18033.9679   1737.376    7441.053\n",
      "  2219.4451  11286.5387   4415.1588   3981.9768   1632.03625 15555.18875\n",
      " 12609.88702  1253.936    6666.243   14254.6082  19496.71917  2261.5688\n",
      "  8932.084   10736.87075  5976.8311   7147.105   43813.8661   9563.029\n",
      " 13047.33235 19361.9988  43896.3763   7740.337   36307.7983   1832.094\n",
      "  7144.86265 38282.7495  24603.04837  1731.677    1632.56445  1631.8212\n",
      "  9991.03765  7935.29115 14043.4767   5934.3798  18972.495   14313.8463\n",
      " 13607.36875 10594.50155  1719.4363   7954.517    1727.54     6686.4313\n",
      "  7152.6714   3877.30425 33471.97189 11879.10405  7804.1605   5969.723\n",
      "  9964.06     9957.7216   9447.3824   1826.843    3704.3545  49577.6624\n",
      " 11946.6259   2904.088    6128.79745  1534.3045  38709.176   10436.096\n",
      " 11073.176    5469.0066   5152.134    3556.9223  22412.6485  25678.77845\n",
      "  3353.4703   6555.07035  2416.955   14590.63205  3732.6251   5846.9176\n",
      " 12730.9996  13616.3586   8988.15875  7650.77375  3594.17085 18246.4955\n",
      "  2155.6815   8569.8618   7526.70645  9222.4026  14119.62    47269.854\n",
      "  3260.199    2709.1119   6933.24225  9264.797    7243.8136   2134.9015\n",
      " 11520.09985  8233.0975   6289.7549   7371.772   12094.478   23563.01618\n",
      "  6457.8434   1615.7667   6600.20595  7046.7222   1984.4533  11455.28\n",
      "  4137.5227  23244.7902  17128.42608  3987.926    4670.64    47291.055\n",
      " 10796.35025 35595.5898   1136.3994  38998.546    2459.7201  21195.818\n",
      " 12268.63225  4827.90495  1635.73365  1969.614    4357.04365 10795.93733\n",
      " 17081.08    13887.9685   3579.8287  14001.2867  47462.894    6753.038\n",
      " 12096.6512  10214.636   17361.7661   1639.5631   8342.90875  4074.4537\n",
      "  8083.9198   2026.9741  31620.00106  1981.5819  20781.48892 24869.8368\n",
      " 18806.14547  8551.347   18218.16139 14210.53595 10959.6947   4441.21315\n",
      " 12142.5786  10560.4917   2897.3235   2755.02095 34617.84065 22331.5668\n",
      "  1625.43375 48517.56315  8219.2039   6940.90985  8240.5896  40103.89\n",
      " 42983.4585  44202.6536   2136.88225  5227.98875]\n",
      "\n",
      "X_val: \n",
      " [[0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "y_val: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ 4795.6568  37701.8768   9800.8882  16138.76205 52590.82939 14133.03775\n",
      "  7639.41745  3736.4647   9290.1395  21223.6758   3866.8552   6079.6715\n",
      "  6640.54485  4428.88785  4040.55825  9140.951   20009.63365 25992.82104\n",
      " 20277.80751  2727.3951  11512.405    8547.6913  12231.6136  21797.0004\n",
      "  6610.1097  37465.34375  1633.9618   3500.6123   5488.262    9715.841\n",
      " 21774.32215  2207.69745 15019.76005 47928.03    18903.49141  6360.9936\n",
      "  2396.0959  43254.41795  1261.859    7281.5056  34828.654    2643.2685\n",
      " 13822.803   10713.644    9910.35985 45863.205    8582.3023   8068.185\n",
      " 12233.828    4134.08245  5003.853   14235.072   12638.195    5246.047\n",
      "  2128.43105 11187.6567   6775.961   10450.552   37270.1512  12244.531\n",
      "  4646.759   21098.55405 36021.0112   5327.40025  1917.3184  19594.80965\n",
      " 25081.76784 27000.98473  7418.522   13041.921   41097.16175  4454.40265\n",
      "  5972.378    5966.8874  11085.5868   8280.6227   9432.9253  24520.264\n",
      "  4449.462    2250.8352  11763.0009  15828.82173  6059.173    6334.34355\n",
      "  8310.83915 29330.98315  6986.697   10141.1362   2699.56835  4133.64165\n",
      "  1639.5631  14349.8544  18608.262   12950.0712   2597.779   26140.3603\n",
      " 12404.8791  12124.9924  22493.65964  2639.0429   3172.018    5438.7491\n",
      "  6393.60345  1837.237   24535.69855  2497.0383   8522.003    4889.0368\n",
      "  9249.4952   8539.671   16232.847    4058.71245 19214.70553  4237.12655\n",
      "  9704.66805 42303.69215  4686.3887   4402.233    3943.5954  11264.541\n",
      "  8930.93455 11842.62375 12949.1554   5415.6612   2483.736    5699.8375\n",
      " 16577.7795  20177.67113 14319.031   40974.1649  19023.26    10601.63225\n",
      " 22462.04375  9225.2564  11554.2236  24915.04626  1725.5523   2457.502\n",
      "  3385.39915 10118.424   20878.78443 38415.474   10848.1343  10928.849\n",
      "  4391.652    6389.37785 10965.446    2585.269   29141.3603   3558.62025\n",
      "  4536.259    1242.816    6313.759   28340.18885  4433.9159   9500.57305\n",
      " 27941.28758 12044.342    3077.0955   2527.81865 45710.20785  8124.4084\n",
      "  6593.5083   8527.532    8520.026   35069.37452  5080.096    4529.477\n",
      " 11163.568    7160.3303   2523.1695  10435.06525 13937.6665   1980.07\n",
      " 18157.876   14001.1338   1146.7966  44260.7499  11576.13    13012.20865\n",
      "  4076.497   11253.421   43753.33705  3208.787   13019.16105 21348.706\n",
      " 11362.755   13470.86     3537.703    1824.2854   1141.4451   1263.249\n",
      " 41676.0811   7448.40395  7421.19455  6186.127    9630.397   11482.63485\n",
      "  6196.448   20149.3229   3443.064    4992.3764  40182.246    6474.013\n",
      "  4738.2682   2156.7518  42124.5153   2130.6759  20462.99766  8017.06115\n",
      " 11488.31695 20709.02034 30063.58055 11938.25595]\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------\n",
    "# SPLITTING DATASET INTO TRAINING, VALIDATION & TESTING |\n",
    "# ------------------------------------------------------\n",
    "# Splits arrays or matrices into random train and test subsets\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = model_selection.train_test_split(X_train, y_train, test_size=0.2, random_state=1)\n",
    "# test_size represents the proportion of the dataset to include in the test split\n",
    "# random_state is the seed used by the random number generator\n",
    "\n",
    "print('\\nSplitting Dataset:'\n",
    "      '\\n==================')\n",
    "print('X_train: \\n', X_train)\n",
    "print('\\ny_train: \\n', y_train)\n",
    "print('\\nX_test: \\n', X_test)\n",
    "print('\\ny_test: \\n', y_test)\n",
    "print('\\nX_val: \\n', X_val)\n",
    "print('\\ny_val: \\n', y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross Validation:\n",
      "=================\n",
      "X_test: \n",
      " [[0. 1. 0. ... 0. 0. 1.]\n",
      " [1. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "Cross Validation:\n",
      "=================\n",
      "X_test: \n",
      " [[0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "Cross Validation:\n",
      "=================\n",
      "X_test: \n",
      " [[0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [1. 0. 0. ... 0. 1. 0.]]\n",
      "\n",
      "Cross Validation:\n",
      "=================\n",
      "X_test: \n",
      " [[0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "\n",
      "Cross Validation:\n",
      "=================\n",
      "X_test: \n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "# Cross Validation |\n",
    "# -----------------\n",
    "# Provides train/trest indices to split data in train.test sets\n",
    "# Each fold is then used once as a validation while the k-1 remaining folds from the training set\n",
    "kf = KFold(n_splits=5)  # number of folds is 5\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print('\\nCross Validation:'\n",
    "          '\\n=================')\n",
    "    print('X_test: \\n', X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.2307245015338618e+17\n",
      "[-4.32291072e+16 -4.32291072e+16 -4.32291072e+16 -4.32291072e+16\n",
      " -4.32291072e+16 -4.32291072e+16 -4.32291072e+16 -4.32291072e+16\n",
      " -4.32291072e+16 -4.32291072e+16 -4.32291072e+16 -4.32291072e+16\n",
      " -4.32291072e+16 -4.32291072e+16 -4.32291072e+16 -4.32291072e+16\n",
      " -4.32291072e+16 -4.32291072e+16 -4.32291072e+16 -4.32291072e+16\n",
      " -4.32291072e+16 -4.32291072e+16 -4.32291072e+16 -4.32291072e+16\n",
      " -4.32291072e+16 -4.32291072e+16 -4.32291072e+16 -4.32291072e+16\n",
      " -4.32291072e+16 -4.32291072e+16 -4.32291072e+16 -4.32291072e+16\n",
      " -4.32291072e+16 -4.32291072e+16 -4.32291072e+16 -4.32291072e+16\n",
      " -4.32291072e+16 -4.32291072e+16 -4.32291072e+16 -4.32291072e+16\n",
      " -4.32291072e+16 -4.32291072e+16 -4.32291072e+16 -4.32291072e+16\n",
      " -4.32291072e+16 -4.32291072e+16 -4.32291072e+16  1.10981199e+17\n",
      "  1.10981199e+17 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15  2.52147324e+17\n",
      " -1.75720683e+15 -1.75720683e+15 -5.88670462e+16 -1.75720683e+15\n",
      " -1.75720683e+15 -7.64201570e+16 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15  4.66527899e+16 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15  6.54254755e+17 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15  8.18773198e+16\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -3.21498618e+17 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -6.27606109e+16\n",
      " -1.75720683e+15 -1.75720683e+15 -3.10642410e+17  4.10999695e+16\n",
      " -1.75720683e+15  1.39112555e+17 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15  2.46226736e+17\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15  3.13616205e+17\n",
      " -1.75720683e+15 -1.75720683e+15  1.16792566e+17 -1.75720683e+15\n",
      " -4.59081321e+16 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.33596428e+17\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -4.24860076e+16 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15  4.75147463e+17 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -3.09973744e+17 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -9.98694395e+16 -3.47388638e+17\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -2.69076047e+17 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15  2.60473623e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -2.56324397e+16\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.86379678e+17\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15  2.00818713e+17 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.09236576e+17 -8.31252970e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.25045537e+17 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -4.89029823e+16 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -5.36855336e+16 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.49736928e+17 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -7.76409298e+16\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15  1.67754085e+17 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -2.86549163e+16 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15  5.45746012e+16 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15  1.42940600e+17 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.12549969e-11 -1.75720683e+15\n",
      " -1.75720683e+15  1.40971679e-11 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      "  1.81898940e-12 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.47792889e-12 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -9.09494702e-13 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15  0.00000000e+00  2.04636308e-12 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -5.45696821e-12\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      "  3.63797881e-12 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -7.27595761e-12\n",
      " -1.75720683e+15 -1.75720683e+15  4.54747351e-12 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15  1.42108547e-12 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -2.72848411e-12\n",
      " -1.75720683e+15 -1.75720683e+15 -1.75720683e+15 -1.75720683e+15\n",
      " -1.36424205e-12 -2.65619006e+17 -2.65619006e+17 -2.65619006e+17\n",
      " -2.65619006e+17 -2.65619006e+17 -2.65619006e+17  1.77063052e+17\n",
      "  1.77063052e+17  2.45633519e+17  2.45633519e+17  2.45633519e+17\n",
      "  2.45633519e+17]\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------\n",
    "# FITTING MULTIPLE REGRESSION MODELS TO THE TRAINGING SET |\n",
    "# --------------------------------------------------------\n",
    "# Fits a linear model with coefficients w to minimize the residual sum of squares between\n",
    "# the observed targets in the dataset, and the targets predicted by the linear approximation\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "print(regressor.intercept_)\n",
    "# retrieve the intercept\n",
    "\n",
    "print(regressor.coef_)\n",
    "# retrieves the slope (coefficient of X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting test set results\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 1. 0. 0.]]\n",
      "          Actual     Predicted\n",
      "0    13974.45555  7.104000e+03\n",
      "1     1909.52745  1.408000e+03\n",
      "2    12096.65120  1.459200e+04\n",
      "3    13204.28565  9.248000e+03\n",
      "4     4562.84210  4.992000e+03\n",
      "..           ...           ...\n",
      "262  10600.54830 -1.232883e+17\n",
      "263   2205.98080  7.872000e+03\n",
      "264   1629.83350  5.376000e+03\n",
      "265   2007.94500  2.240000e+02\n",
      "266  29141.36030  2.793600e+04\n",
      "\n",
      "[267 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# PREDICTING THE TEST RESULTS |\n",
    "# ----------------------------\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "print('Predicting test set results')\n",
    "print(X_test)\n",
    "\n",
    "dataset = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# METHOD OF FEATURE SCALING |\n",
    "# --------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "StandardScalar:\n",
      "===============\n",
      "X_train:\n",
      " [[-0.22589914  4.17917662 -0.13794744 ... -0.55757826 -0.61656682\n",
      "   1.76613441]\n",
      " [ 4.42675432 -0.23928158 -0.13794744 ... -0.55757826  1.62188424\n",
      "  -0.56620832]\n",
      " [-0.22589914 -0.23928158 -0.13794744 ... -0.55757826  1.62188424\n",
      "  -0.56620832]\n",
      " ...\n",
      " [-0.22589914 -0.23928158 -0.13794744 ...  1.79347022 -0.61656682\n",
      "  -0.56620832]\n",
      " [-0.22589914 -0.23928158 -0.13794744 ... -0.55757826  1.62188424\n",
      "  -0.56620832]\n",
      " [-0.22589914 -0.23928158 -0.13794744 ... -0.55757826  1.62188424\n",
      "  -0.56620832]]\n",
      "\n",
      "X_test: \n",
      " [[-0.22589914 -0.23928158 -0.13794744 ... -0.55757826 -0.61656682\n",
      "  -0.56620832]\n",
      " [-0.22589914 -0.23928158 -0.13794744 ...  1.79347022 -0.61656682\n",
      "  -0.56620832]\n",
      " [-0.22589914 -0.23928158 -0.13794744 ... -0.55757826 -0.61656682\n",
      "  -0.56620832]\n",
      " ...\n",
      " [ 4.42675432 -0.23928158 -0.13794744 ... -0.55757826  1.62188424\n",
      "  -0.56620832]\n",
      " [-0.22589914 -0.23928158 -0.13794744 ... -0.55757826 -0.61656682\n",
      "   1.76613441]\n",
      " [-0.22589914 -0.23928158 -0.13794744 ...  1.79347022 -0.61656682\n",
      "  -0.56620832]]\n"
     ]
    }
   ],
   "source": [
    "# ----------------\n",
    "#  STANDARDSCALER |\n",
    "# ----------------\n",
    "# Computes mean and standard deviation on a training set so as to be able to later\n",
    "# reapply the same transformation on the testing set.\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "\n",
    "print(\"\\nStandardScalar:\"\n",
    "      \"\\n===============\"\n",
    "      \"\\nX_train:\\n\", X_train)\n",
    "print('\\nX_test: \\n', X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MinMaxScalar:\n",
      "=============\n",
      "X_train:\n",
      " [[0. 1. 0. ... 0. 0. 1.]\n",
      " [1. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "\n",
      "X_test:\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# --------------\n",
    "#  MINMAXSCALAR |\n",
    "# --------------\n",
    "# Scaling features to lie between a given minimum and maximum value, often between 0 and 1\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_train = min_max_scaler.fit_transform(X_train)\n",
    "X_test = min_max_scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nMinMaxScalar:\"\n",
    "      \"\\n=============\"\n",
    "      \"\\nX_train:\\n\", X_train)\n",
    "print('\\nX_test:\\n', X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RobustScalar:\n",
      "=============\n",
      "X_train:\n",
      " [[0. 1. 0. ... 0. 0. 1.]\n",
      " [1. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "\n",
      "X_test:\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# --------------\n",
    "#  ROBUSTSCALAR |\n",
    "# --------------\n",
    "# This removed the median and scaled the data according to the quantile range\n",
    "robust_scaler = RobustScaler()\n",
    "X_train = robust_scaler.fit_transform(X_train)\n",
    "X_test = robust_scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nRobustScalar:\"\n",
    "      \"\\n=============\"\n",
    "      \"\\nX_train:\\n\", X_train)\n",
    "print('\\nX_test:\\n', X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Normalizer:\n",
      "===========\n",
      "X_train:\n",
      " [[0.         0.35355339 0.         ... 0.         0.         0.35355339]\n",
      " [0.5        0.         0.         ... 0.         0.5        0.        ]\n",
      " [0.         0.         0.         ... 0.         0.5        0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.5        0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.40824829 0.        ]\n",
      " [0.         0.         0.         ... 0.         0.40824829 0.        ]]\n",
      "\n",
      "X_test:\n",
      " [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.5        0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.40824829 0.         0.         ... 0.         0.40824829 0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.40824829]\n",
      " [0.         0.         0.         ... 0.35355339 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# --------------\n",
    "#  NORMALIZER   |\n",
    "# --------------\n",
    "# Normalize samples individually to unit norm\n",
    "# Each sample (each row of the data matrix) with at least one non zero component is rescaled\n",
    "# indepentently o other samples so that its norm (|1 or |2) equals 1\n",
    "normalizer_scaler = Normalizer()\n",
    "X_train = normalizer_scaler.fit_transform(X_train)\n",
    "X_test = normalizer_scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nNormalizer:\"\n",
    "      \"\\n===========\"\n",
    "      \"\\nX_train:\\n\", X_train)\n",
    "print('\\nX_test:\\n', X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                             OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.037\n",
      "Model:                            OLS   Adj. R-squared:                  0.033\n",
      "Method:                 Least Squares   F-statistic:                     8.525\n",
      "Date:                Wed, 25 Mar 2020   Prob (F-statistic):           4.15e-09\n",
      "Time:                        21:45:25   Log-Likelihood:                -14452.\n",
      "No. Observations:                1338   AIC:                         2.892e+04\n",
      "Df Residuals:                    1331   BIC:                         2.896e+04\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       1.429e+04    361.092     39.578      0.000    1.36e+04     1.5e+04\n",
      "x1         -7204.9744   1478.632     -4.873      0.000   -1.01e+04   -4304.270\n",
      "x2         -4543.2826   1488.821     -3.052      0.002   -7463.975   -1622.591\n",
      "x3         -4131.4942   2241.020     -1.844      0.065   -8527.810     264.822\n",
      "x4         -9560.7276   2279.666     -4.194      0.000    -1.4e+04   -5088.598\n",
      "x5         -4278.2591   2279.666     -1.877      0.061   -8750.389     193.870\n",
      "x6         -1871.3719   2279.666     -0.821      0.412   -6343.501    2600.758\n",
      "==============================================================================\n",
      "Omnibus:                      353.344   Durbin-Watson:                   2.012\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              704.068\n",
      "Skew:                           1.574   Prob(JB):                    1.30e-153\n",
      "Kurtosis:                       4.650   Cond. No.                         7.30\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "                             OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.037\n",
      "Model:                            OLS   Adj. R-squared:                  0.033\n",
      "Method:                 Least Squares   F-statistic:                     10.10\n",
      "Date:                Wed, 25 Mar 2020   Prob (F-statistic):           1.66e-09\n",
      "Time:                        21:45:25   Log-Likelihood:                -14453.\n",
      "No. Observations:                1338   AIC:                         2.892e+04\n",
      "Df Residuals:                    1332   BIC:                         2.895e+04\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       1.424e+04    356.490     39.957      0.000    1.35e+04    1.49e+04\n",
      "x1         -7158.0224   1477.345     -4.845      0.000   -1.01e+04   -4259.846\n",
      "x2         -4496.3306   1487.540     -3.023      0.003   -7414.507   -1578.154\n",
      "x3         -4084.5422   2240.015     -1.823      0.068   -8478.885     309.800\n",
      "x4         -9513.7756   2278.669     -4.175      0.000    -1.4e+04   -5043.604\n",
      "x5         -4231.3072   2278.669     -1.857      0.064   -8701.478     238.864\n",
      "==============================================================================\n",
      "Omnibus:                      352.614   Durbin-Watson:                   2.013\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              702.111\n",
      "Skew:                           1.570   Prob(JB):                    3.46e-153\n",
      "Kurtosis:                       4.653   Cond. No.                         7.19\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "                             OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.034\n",
      "Model:                            OLS   Adj. R-squared:                  0.031\n",
      "Method:                 Least Squares   F-statistic:                     11.77\n",
      "Date:                Wed, 25 Mar 2020   Prob (F-statistic):           2.13e-09\n",
      "Time:                        21:45:25   Log-Likelihood:                -14454.\n",
      "No. Observations:                1338   AIC:                         2.892e+04\n",
      "Df Residuals:                    1333   BIC:                         2.894e+04\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       1.414e+04    352.253     40.144      0.000    1.34e+04    1.48e+04\n",
      "x1         -7054.5711   1477.542     -4.775      0.000   -9953.132   -4156.010\n",
      "x2         -4392.8793   1487.753     -2.953      0.003   -7311.473   -1474.286\n",
      "x3         -9410.3243   2279.948     -4.127      0.000   -1.39e+04   -4937.646\n",
      "x4         -4127.8559   2279.948     -1.811      0.070   -8600.534     344.822\n",
      "==============================================================================\n",
      "Omnibus:                      351.304   Durbin-Watson:                   2.011\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              698.660\n",
      "Skew:                           1.564   Prob(JB):                    1.94e-152\n",
      "Kurtosis:                       4.657   Cond. No.                         7.12\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "                             OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.022\n",
      "Model:                            OLS   Adj. R-squared:                  0.020\n",
      "Method:                 Least Squares   F-statistic:                     9.897\n",
      "Date:                Wed, 25 Mar 2020   Prob (F-statistic):           1.87e-06\n",
      "Time:                        21:45:25   Log-Likelihood:                -14463.\n",
      "No. Observations:                1338   AIC:                         2.893e+04\n",
      "Df Residuals:                    1334   BIC:                         2.895e+04\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       1.392e+04    350.109     39.748      0.000    1.32e+04    1.46e+04\n",
      "x1         -6829.9427   1485.387     -4.598      0.000   -9743.892   -3915.993\n",
      "x2         -4168.2510   1495.667     -2.787      0.005   -7102.367   -1234.135\n",
      "x3         -3903.2275   2292.957     -1.702      0.089   -8401.422     594.967\n",
      "==============================================================================\n",
      "Omnibus:                      350.349   Durbin-Watson:                   2.007\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              695.369\n",
      "Skew:                           1.561   Prob(JB):                    1.01e-151\n",
      "Kurtosis:                       4.650   Cond. No.                         7.02\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "                             OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.020\n",
      "Model:                            OLS   Adj. R-squared:                  0.018\n",
      "Method:                 Least Squares   F-statistic:                     13.38\n",
      "Date:                Wed, 25 Mar 2020   Prob (F-statistic):           1.77e-06\n",
      "Time:                        21:45:25   Log-Likelihood:                -14464.\n",
      "No. Observations:                1338   AIC:                         2.893e+04\n",
      "Df Residuals:                    1335   BIC:                         2.895e+04\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       1.383e+04    346.250     39.928      0.000    1.31e+04    1.45e+04\n",
      "x1         -6738.9433   1485.480     -4.537      0.000   -9653.072   -3824.814\n",
      "x2         -4077.2515   1495.773     -2.726      0.006   -7011.574   -1142.929\n",
      "==============================================================================\n",
      "Omnibus:                      345.862   Durbin-Watson:                   2.012\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              680.615\n",
      "Skew:                           1.547   Prob(JB):                    1.61e-148\n",
      "Kurtosis:                       4.624   Cond. No.                         4.69\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "                             OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.014\n",
      "Model:                            OLS   Adj. R-squared:                  0.013\n",
      "Method:                 Least Squares   F-statistic:                     19.23\n",
      "Date:                Wed, 25 Mar 2020   Prob (F-statistic):           1.25e-05\n",
      "Time:                        21:45:25   Log-Likelihood:                -14468.\n",
      "No. Observations:                1338   AIC:                         2.894e+04\n",
      "Df Residuals:                    1336   BIC:                         2.895e+04\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       1.361e+04    337.655     40.298      0.000    1.29e+04    1.43e+04\n",
      "x1         -6520.4617   1486.881     -4.385      0.000   -9437.338   -3603.586\n",
      "==============================================================================\n",
      "Omnibus:                      341.689   Durbin-Watson:                   2.007\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              668.753\n",
      "Skew:                           1.530   Prob(JB):                    6.06e-146\n",
      "Kurtosis:                       4.624   Cond. No.                         4.53\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# OPTIMAL MODEL USING BACKWARD ELIMINATION (FEATURE SELECTION) |\n",
    "# -------------------------------------------------------------\n",
    "import statsmodels.api as sm\n",
    "X = np.append(arr=np.ones((1338, 1)).astype(int), values=X, axis=1)\n",
    "# add one column with all 50 values as 1 to represent b0x0\n",
    "\n",
    "X_opt = X[:, [0, 1, 2, 3, 4, 5, 6]]\n",
    "regressor_OLS = sm.OLS(endog=y, exog=X_opt).fit()\n",
    "# endog is the dependent variable & exog is the independent variable\n",
    "print('\\n', regressor_OLS.summary())\n",
    "\n",
    "# remove index x6 as it has the highest p-value\n",
    "X_opt = X[:, [0, 1, 2, 3, 4, 5]]\n",
    "regressor_OLS = sm.OLS(endog=y, exog=X_opt).fit()\n",
    "print('\\n', regressor_OLS.summary())\n",
    "\n",
    "# remove index x3 as it has the highest p-value\n",
    "X_opt = X[:, [0, 1, 2, 4, 5]]\n",
    "regressor_OLS = sm.OLS(endog=y, exog=X_opt).fit()\n",
    "print('\\n', regressor_OLS.summary())\n",
    "\n",
    "# remove index x4 as it has the highest p-value\n",
    "X_opt = X[:, [0, 1, 2, 5]]\n",
    "regressor_OLS = sm.OLS(endog=y, exog=X_opt).fit()\n",
    "print('\\n', regressor_OLS.summary())\n",
    "\n",
    "# remove index x3 as it has the highest p-value\n",
    "X_opt = X[:, [0, 1, 2]]\n",
    "regressor_OLS = sm.OLS(endog=y, exog=X_opt).fit()\n",
    "print('\\n', regressor_OLS.summary())\n",
    "\n",
    "# remove index x2 as it has the highest p-value\n",
    "X_opt = X[:, [0, 1]]\n",
    "regressor_OLS = sm.OLS(endog=y, exog=X_opt).fit()\n",
    "print('\\n', regressor_OLS.summary())\n",
    "# now all variables are under the significance level of 0.05\n",
    "# the only one variable left has the highest impact on the profit and is statistically significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================\n",
    "# DATA PROCESSING |\n",
    "# ================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------\n",
    "# Normalization |\n",
    "# --------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_normalize_column_0_1:\n",
      "======================\n",
      " [[1. 0. 1. ... 0. 0. 1.]\n",
      " [1. 1. 0. ... 0. 1. 0.]\n",
      " [1. 0. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [1. 1. 0. ... 0. 1. 0.]\n",
      " [1. 0. 0. ... 0. 0. 1.]\n",
      " [1. 0. 0. ... 1. 0. 0.]]\n",
      "\n",
      "_normalize_column_0_1:\n",
      "X_max is \n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "X_min is \n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "specified_column is \n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
      " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
      " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
      " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
      " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
      " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
      " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
      " 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
      " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593\n",
      " 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[nan,  0.,  1., ...,  0.,  0.,  1.],\n",
       "        [nan,  1.,  0., ...,  0.,  1.,  0.],\n",
       "        [nan,  0.,  0., ...,  0.,  1.,  0.],\n",
       "        ...,\n",
       "        [nan,  1.,  0., ...,  0.,  1.,  0.],\n",
       "        [nan,  0.,  0., ...,  0.,  0.,  1.],\n",
       "        [nan,  0.,  0., ...,  1.,  0.,  0.]]),\n",
       " array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]]),\n",
       " array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize all input data to 0 and 1\n",
    "def _normalize_column_0_1(X, train=True, specified_column=None, X_min=None, X_max=None):\n",
    "    # The output of the function will make the specified column of the training data from 0 to 1\n",
    "    # When processing testing data,k we need to normalize by the value we used for processing training, so we must\n",
    "    # save the max value of the training data\n",
    "    if train:\n",
    "        if specified_column is None:\n",
    "            specified_column = np.arange(X.shape[1])\n",
    "            # np.arange returns evenly spaced values within a given interval\n",
    "        length = len(specified_column)\n",
    "        # returns the number of items\n",
    "        X_max = np.reshape(np.max(X[:, specified_column], 0), (1, length))\n",
    "        # np.reshape gives a new shape to an array without changing its data\n",
    "        # np.max finds the value of maximum element in the entire array (returning scalar)\n",
    "        X_min = np.reshape(np.min(X[:, specified_column], 0), (1, length))\n",
    "        # np.min returns the minimum of an array or minimum along an axis\n",
    "        print('\\n_normalize_column_0_1:')\n",
    "        print('X_max is \\n' + str(X_max))\n",
    "        print('X_min is \\n' + str(X_min))\n",
    "        print('specified_column is \\n' + str(specified_column))\n",
    "\n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "    # code is trying to divide by zero, this ignores the runtime error\n",
    "\n",
    "    X[:, specified_column] = np.divide(np.subtract(X[:, specified_column], X_min), np.subtract(X_max, X_min))\n",
    "    # np.divide returns a true division of the inputs, element-wise\n",
    "    # np.subtract subtracts arguments, element-wise\n",
    "\n",
    "    return X, X_max, X_min\n",
    "\n",
    "\n",
    "print('_normalize_column_0_1:'\n",
    "      '\\n======================\\n', X)\n",
    "\n",
    "_normalize_column_0_1(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_normalize_column_normal:\n",
      "=========================\n",
      " [[nan  0.  1. ...  0.  0.  1.]\n",
      " [nan  1.  0. ...  0.  1.  0.]\n",
      " [nan  0.  0. ...  0.  1.  0.]\n",
      " ...\n",
      " [nan  1.  0. ...  0.  1.  0.]\n",
      " [nan  0.  0. ...  0.  0.  1.]\n",
      " [nan  0.  0. ...  1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Normalize the specified column to a normal distribution\n",
    "def _normalize_column_normal(X, train=True, specified_column=None, X_mean=None, X_std=None):\n",
    "    # The output of the function will make the specified column number to become a Normal Distribution\n",
    "    # When processing testing data, we need to normalize by the value we used for processing traing, so we must\n",
    "    # save the mean value and the variance of the training data\n",
    "    if train:\n",
    "        if specified_column is None:\n",
    "            specified_column = np.arange(X.shape[0])\n",
    "        length = len(specified_column)\n",
    "        X_mean = np.reshape(np.mean(X[:, specified_column], 0), (1, length))\n",
    "        X_std = np.reshape(np.std(X[:, specified_column], 0), (1, length))\n",
    "\n",
    "    X[:, specified_column] = np.divide(np.subtract(X[:, specified_column], X_mean), X_std)\n",
    "\n",
    "    return X, X_mean, X_std\n",
    "\n",
    "\n",
    "print('\\n_normalize_column_normal:'\n",
    "      '\\n=========================\\n', X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_shuffle: \n",
      "=========\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[nan,  0.,  0., ...,  0.,  0.,  1.],\n",
       "        [nan,  0.,  0., ...,  0.,  0.,  1.],\n",
       "        [nan,  1.,  0., ...,  0.,  0.,  0.],\n",
       "        ...,\n",
       "        [nan,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [nan,  0.,  0., ...,  1.,  0.,  0.],\n",
       "        [nan,  0.,  0., ...,  0.,  0.,  0.]]),\n",
       " array([ 3987.926  ,  7726.854  ,  1702.4553 , ...,  4915.05985,\n",
       "        30284.64294, 42111.6647 ]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Makes the data random and clean and changes the order\n",
    "def _shuffle(X, Y):\n",
    "    randomize = np.arange(len(X))\n",
    "    np.random.shuffle(randomize)\n",
    "    # shuffles the array along the 1st axis of a multi-dimensional array\n",
    "    # the order of sub-arrays is changed but their contents remains the same\n",
    "    return X[randomize], Y[randomize]\n",
    "\n",
    "\n",
    "print('\\n_shuffle: '\n",
    "      '\\n=========\\n')\n",
    "_shuffle(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_dev_split:\n",
      "================\n",
      "[[nan  0.  1. ...  0.  0.  1.]\n",
      " [nan  1.  0. ...  0.  1.  0.]\n",
      " [nan  0.  0. ...  0.  1.  0.]\n",
      " ...\n",
      " [nan  1.  0. ...  0.  1.  0.]\n",
      " [nan  0.  0. ...  0.  0.  1.]\n",
      " [nan  0.  0. ...  1.  0.  0.]] \n",
      " [16884.924   1725.5523  4449.462  ...  1629.8335  2007.945  29141.3603]\n"
     ]
    }
   ],
   "source": [
    "# Divide the data and choose according to the proportion\n",
    "def train_dev_split(X, y, dev_size=0.25):\n",
    "    train_len = int(round(len(X) * (1 - dev_size)))\n",
    "    return X[0:train_len], y[0:train_len], X[train_len:None], y[train_len:None]\n",
    "\n",
    "\n",
    "print('\\ntrain_dev_split:'\n",
    "      '\\n================')\n",
    "print(X, '\\n', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [0,1,3,4,5,7,10,12,25,26,27,28]\n",
    "X_train, X_mean, X_std = _normalize_column_normal(X_train, specified_column=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# LOGISTIC REGRESSION TOOL |\n",
    "# -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _sigmoid(z):\n",
    "    # sigmoid function can be used to output probability\n",
    "    # limits the output to a range between 0 and 1\n",
    "    return np.clip(1 / (1.0 + np.exp(-z)), 1e-6, 1-1e-6)\n",
    "    # 1e-6 is equivalent to 1 with 6 zeros (1,000,000)\n",
    "    \n",
    "_sigmoid(0)\n",
    "# test to make sure it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob(X, w, b):\n",
    "    # the probability to output 1\n",
    "    return _sigmoid(np.add(np.matmul(X, w), b))\n",
    "    # np.matmul matrix product of 2 arrays\n",
    "    # np.add adds arguments element-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(X, w, b):\n",
    "    # use round to infer the result\n",
    "    return np.round(get_prob(X, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cross_entropy(y_pred, y_label):\n",
    "    # compute the cross entropy\n",
    "    # used to quantify the difference between 2 probability distributions\n",
    "    cross_entropy = -np.dot(y_label, np.log(y_pred))-np.dot((1-y_label), np.log(1-y_pred))\n",
    "    # np.dot is the dot product of 2 arrays\n",
    "    # np.log = natural logarithm\n",
    "    return cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gradient(X, y_label, w, b):\n",
    "    # return the mean of the gradient\n",
    "    y_pred = get_prob(X, w, b)\n",
    "    pred_error = y_label - y_pred\n",
    "    w_grad = -np.mean(np.multiply(pred_error.T, X.T), 1)\n",
    "    b_grad = -np.mean(pred_error)\n",
    "    return w_grad, b_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gradient_regularization(X, y_label, w, b, lamda):\n",
    "    # return the mean of the gradient\n",
    "    y_pred = get_prob(X, w, b)\n",
    "    pred_error = y_label - y_pred\n",
    "    w_grad = -np.mean(np.multiply(pred_error.T, X.T), 1) + lamda * w\n",
    "    # .T transposes the array\n",
    "    b_grad = -np.mean(pred_error)\n",
    "    return w_grad, b_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _loss(y_pred, y_label, lamda, w):\n",
    "    return _cross_entropy(y_pred, y_label)+lamda*np.sum(np.square(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_label):\n",
    "    acc = np.sum(y_pred == y_label) / len(y_pred)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Logistic Regression |\n",
    "# --------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, y_train):\n",
    "    # split a validation set\n",
    "    dev_size = 0.1155\n",
    "    X_train, y_train, X_dev, y_dev = train_dev_split(X_train, y_train, dev_size=dev_size)\n",
    "\n",
    "    # Use 0 + 0*x1 + 0*x2 + ... for weight initialization\n",
    "    w = np.zeros((X_train.shape[1],))\n",
    "    b = np.zeros((1,))\n",
    "    regularize = True\n",
    "    if regularize:\n",
    "        lamda = 0.001\n",
    "    else:\n",
    "        lamda = 0\n",
    "\n",
    "    max_iter = 40  # max iteration number\n",
    "    batch_size = 32  # number to feed in the model for average to avoid bias\n",
    "    learning_rate = 0.2  # how much the model learn for each step\n",
    "    num_train = len(y_train)\n",
    "    num_dev = len(y_dev)\n",
    "    step = 1\n",
    "\n",
    "    loss_train = []\n",
    "    loss_validation = []\n",
    "    train_acc = []\n",
    "    dev_acc = []\n",
    "\n",
    "    for epoch in range(max_iter):\n",
    "        # Random shuffle for each epoch\n",
    "        X_train, y_train = _shuffle(X_train, y_train)\n",
    "\n",
    "        total_loss = 0.0\n",
    "        # Logistic regression train with batch\n",
    "        for idx in range(int(np.floor(len(y_train)/batch_size))):\n",
    "            X = X_train[idx*batch_size:(idx+1)*batch_size]\n",
    "            y = y_train[idx*batch_size:(idx+1)*batch_size]\n",
    "\n",
    "            # Find out the gradient of the loss\n",
    "            w_grad, b_grad = _gradient_regularization(X, y, w, b, lamda)\n",
    "            # gradient descent update\n",
    "            # learning rate decay with time\n",
    "            w = w - learning_rate/np.sqrt(step) * w_grad\n",
    "            b = b - learning_rate/np.sqrt(step) * b_grad\n",
    "\n",
    "            step = step+1\n",
    "\n",
    "        # Compute the loss and the accuracy of the training set and the validation set\n",
    "        y_train_pred = get_prob(X_train, w, b)\n",
    "        y_train_pred = np.round(y_train_pred)\n",
    "        train_acc.append(accuracy(y_train_pred, y_train))\n",
    "        loss_train.append(_loss(y_train_pred, y_train, lamda, w)/num_train)\n",
    "        y_dev_pred = get_prob(X_dev, w, b)\n",
    "        y_dev_pred = np.round(y_dev_pred)\n",
    "        dev_acc.append(accuracy(y_dev_pred, y_dev))\n",
    "        loss_validation.append(_loss(y_dev_pred, y_dev, lamda, w)/num_dev)\n",
    "\n",
    "    return w, b, loss_train, loss_validation, train_acc, dev_acc  # return loss for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return loss is to plot the result\n",
    "w, b, loss_train, loss_validation, train_acc, dev_acc = train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_train)\n",
    "plt.plot(loss_validation)\n",
    "plt.legend(['train', 'dev'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_acc)\n",
    "plt.plot(dev_acc)\n",
    "plt.legend(['train', 'dev'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# -----------------------------\n",
    "#  TEST DATA AND OUTPUT RESULT |\n",
    "# -----------------------------\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nX_train_fpath = sys.argv[1]\\nY_train_fpath = sys.argv[2]\\nX_test_fpath = sys.argv[3]\\noutput_fpath = sys.argv[4]\\n\\n\\nX_train_fpath = 'data/X_train'\\nY_train_fpath = 'data/Y_train'\\nX_test_fpath = 'data/X_test'\\noutput_fpath = 'output.csv'\\n\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "X_train_fpath = sys.argv[1]\n",
    "Y_train_fpath = sys.argv[2]\n",
    "X_test_fpath = sys.argv[3]\n",
    "output_fpath = sys.argv[4]\n",
    "\n",
    "\n",
    "X_train_fpath = 'data/X_train'\n",
    "Y_train_fpath = 'data/Y_train'\n",
    "X_test_fpath = 'data/X_test'\n",
    "output_fpath = 'output.csv'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.genfromtxt(dataset, delimiter=',', skip_header=1)\n",
    "Y_train = np.genfromtxt(dataset, delimiter=',', skip_header=1)\n",
    "# np.genfromtxt loads data from a text file, with missing values handled as specified\n",
    "# each line past the first skip_header line is split at the delimiter character and\n",
    "# characters following the comments characters are discarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-706d0324e819>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_header\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Do the same data process to the test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_normalize_column_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspecified_column\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_mean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_std\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_std\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-4aad567d5f1d>\u001b[0m in \u001b[0;36m_normalize_column_normal\u001b[1;34m(X, train, specified_column, X_mean, X_std)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mX_std\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspecified_column\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspecified_column\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdivide\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspecified_column\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_mean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_std\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_std\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "X_test = np.genfromtxt(dataset, delimiter=',', skip_header=1)\n",
    "# Do the same data process to the test data\n",
    "X_test, _, _ = _normalize_column_normal(X_test, train=False, specified_column=col, X_mean=X_mean, X_std=X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = infer(X_test, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_fpath, 'w') as f:\n",
    "    f.write('id,label\\n')\n",
    "    for i, v in enumerate(result):\n",
    "        f.write('%d,%d\\n' % (i+1, v))\n",
    "\n",
    "ind = np.argsort(np.abs(w))[::-1]\n",
    "with open(X_test_fpath) as f:\n",
    "    content = f.readline().rstrip('\\n')\n",
    "features = np.array([x for x in content.split(',')])\n",
    "for i in ind[0:10]:\n",
    "    print(features[i], w[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
